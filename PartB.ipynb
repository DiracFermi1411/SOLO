{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# download data, you can maunally download dataset set here:\n",
        "# https://drive.google.com/drive/folders/1eP7FtPaWfJ5zLdcsZYl6eyn5EYixkFn8\n",
        "\n",
        "!gdown --id 1JD3OaHpq_4KCb7ofcPMkknmdEXFHrCcn\n",
        "!gdown --id 1ssRA7yijGLFmJU-ac-lPyUOq7DYzTAS1\n",
        "!gdown --id 1Rpz-ZuQxDwvLyzc0FD9GZxAKlyka3VC5\n",
        "!gdown --id 1ouMFNT1thia8l6P5vcWCY-nweAexLDsB\n",
        "!gdown --id 1Xyo9voEtL8fFf0gh7NY8L0cC6z8mZDrl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-66pgeZjTq2",
        "outputId": "b8cfaf0b-c238-42ca-a521-5ee4247d9d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JD3OaHpq_4KCb7ofcPMkknmdEXFHrCcn\n",
            "To: /content/hw3_mycocodata_mask_comp_zlib.h5\n",
            "100% 4.30M/4.30M [00:00<00:00, 29.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ssRA7yijGLFmJU-ac-lPyUOq7DYzTAS1\n",
            "To: /content/hw3_mycocodata_labels_comp_zlib.npy\n",
            "100% 269k/269k [00:00<00:00, 4.99MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rpz-ZuQxDwvLyzc0FD9GZxAKlyka3VC5\n",
            "To: /content/hw3_mycocodata_img_comp_zlib.h5\n",
            "100% 801M/801M [00:08<00:00, 96.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ouMFNT1thia8l6P5vcWCY-nweAexLDsB\n",
            "To: /content/hw3_mycocodata_bboxes_comp_zlib.npy\n",
            "100% 327k/327k [00:00<00:00, 4.07MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1Xyo9voEtL8fFf0gh7NY8L0cC6z8mZDrl \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torchtext\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs7ph_GCjZjl",
        "outputId": "18d40df7-71d0-48c2-8e7f-64679ca95ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.0.9.post0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (17.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.6)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch import nn, optim\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.loggers as pl_loggers\n",
        "import pytorch_lightning.callbacks as pl_callbacks\n",
        "\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from solo_head import *\n",
        "from backbone import *\n",
        "from dataset import *\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from tensorboardX import SummaryWriter\n",
        "import os.path\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "root_dir = \"/content/\""
      ],
      "metadata": {
        "id": "m_zTaql6jlQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device = \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGxJH-X4k91q",
        "outputId": "2f5386d5-a408-4f54-b69d-62f19a4290fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths = ['hw3_mycocodata_img_comp_zlib.h5',\n",
        "         'hw3_mycocodata_mask_comp_zlib.h5',\n",
        "         'hw3_mycocodata_labels_comp_zlib.npy',\n",
        "         'hw3_mycocodata_bboxes_comp_zlib.npy']\n",
        "\n",
        "dataset = BuildDataset(paths)\n",
        "del paths"
      ],
      "metadata": {
        "id": "XQowDxorjywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "torch.random.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, test_size])\n",
        "\n",
        "batch_size = 2\n",
        "train_build_loader = BuildDataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "train_loader = train_build_loader.loader()\n",
        "test_build_loader = BuildDataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = test_build_loader.loader()"
      ],
      "metadata": {
        "id": "8DA0ITvjj1gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_fpn = Resnet50Backbone()\n",
        "solo_head = SOLOHead(num_classes=4)\n",
        "\n",
        "resnet50_fpn = resnet50_fpn.to(device)\n",
        "resnet50_fpn.eval()\n",
        "solo_head = solo_head.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "optimizer = optim.SGD(solo_head.parameters(), lr=0.01 /\n",
        "                      16*batch_size, momentum=0.9, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=[27, 33], gamma=0.1)"
      ],
      "metadata": {
        "id": "upohc37Tll_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=\"logs\")\n",
        "\n",
        "os.makedirs(\"train_checkpt\", exist_ok=True)"
      ],
      "metadata": {
        "id": "E2qjtEN8l9iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "train_losses = {\"cate\": [], \"mask\": [], \"total\": []}\n",
        "test_losses = {\"cate\": [], \"mask\": [], \"total\": []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    solo_head.train()\n",
        "    cum_losses = {\"cate\": 0.0, \"mask\": 0.0, \"total\": 0.0}\n",
        "\n",
        "    for iter, data in enumerate(train_loader):\n",
        "        img, label_list, mask_list, bbox_list = data\n",
        "        img, label_list, mask_list, bbox_list = img.to(device), [x.to(device) for x in label_list], [x.to(device) for x in mask_list], [x.to(device) for x in bbox_list]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            backout = resnet50_fpn(img)\n",
        "\n",
        "        fpn_feat_list = list(backout.values())\n",
        "        optimizer.zero_grad()\n",
        "        cate_pred_list, ins_pred_list = solo_head.forward(fpn_feat_list, eval=False)\n",
        "\n",
        "        ins_gts_list, ins_ind_gts_list, cate_gts_list = solo_head.target(ins_pred_list, bbox_list, label_list, mask_list)\n",
        "        cate_loss, mask_loss, total_loss = solo_head.loss(cate_pred_list, ins_pred_list, ins_gts_list, ins_ind_gts_list, cate_gts_list)\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        cum_losses[\"cate\"] += cate_loss.item()\n",
        "        cum_losses[\"mask\"] += mask_loss.item()\n",
        "        cum_losses[\"total\"] += total_loss.item()\n",
        "\n",
        "        if np.isnan(cum_losses[\"total\"]):\n",
        "            raise RuntimeError(f\"[ERROR] NaN encountered - iter {iter}\")\n",
        "\n",
        "        if iter % 100 == 99:\n",
        "            log_losses = {key: cum_losses[key] / 100.0 for key in cum_losses}\n",
        "            for loss_type in [\"cate\", \"mask\", \"total\"]:\n",
        "                writer.add_scalar(f'Loss/train/log_{loss_type}_loss', log_losses[loss_type], len(train_losses[loss_type]))\n",
        "                train_losses[loss_type].append(log_losses[loss_type])\n",
        "            print(f'\\nIteration:{iter+1} Avg. train total loss: {log_losses[\"total\"]:.4f}')\n",
        "            cum_losses = {key: 0.0 for key in cum_losses}\n",
        "\n",
        "    path = f'./train_checkpt/solo_epoch_{epoch}'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': solo_head.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "\n",
        "    # Validation\n",
        "    solo_head.eval()\n",
        "    test_cum_losses = {\"cate\": 0.0, \"mask\": 0.0, \"total\": 0.0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for iter, data in enumerate(test_loader):\n",
        "            img, label_list, mask_list, bbox_list = data\n",
        "            img, label_list, mask_list, bbox_list = img.to(device), [x.to(device) for x in label_list], [x.to(device) for x in mask_list], [x.to(device) for x in bbox_list]\n",
        "\n",
        "            backout = resnet50_fpn(img)\n",
        "            fpn_feat_list = list(backout.values())\n",
        "            cate_pred_list, ins_pred_list = solo_head.forward(fpn_feat_list, eval=False)\n",
        "\n",
        "            ins_gts_list, ins_ind_gts_list, cate_gts_list = solo_head.target(ins_pred_list, bbox_list, label_list, mask_list)\n",
        "            cate_loss, mask_loss, total_loss = solo_head.loss(cate_pred_list, ins_pred_list, ins_gts_list, ins_ind_gts_list, cate_gts_list)\n",
        "            test_cum_losses[\"cate\"] += cate_loss.item()\n",
        "            test_cum_losses[\"mask\"] += mask_loss.item()\n",
        "            test_cum_losses[\"total\"] += total_loss.item()\n",
        "\n",
        "        epoch_losses = {key: test_cum_losses[key] / len(test_loader.dataset) for key in test_cum_losses}\n",
        "        print(f'\\nEpoch:{epoch + 1} Avg. test loss: {epoch_losses[\"total\"]:.4f}')\n",
        "\n",
        "        for loss_type in [\"cate\", \"mask\", \"total\"]:\n",
        "            writer.add_scalar(f'Loss/test/{loss_type}_loss', epoch_losses[loss_type], epoch)\n",
        "            test_losses[loss_type].append(epoch_losses[loss_type])\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "CsBF4pbJpRoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, tracker_id):\n",
        "        self.reset()\n",
        "        self.tracker_id = tracker_id\n",
        "\n",
        "    def reset(self):\n",
        "        self.conf_scores = np.zeros((0,))\n",
        "        self.tp_indicator = np.zeros((0,), np.bool_)\n",
        "        self.match_idx = np.zeros((0,), np.int32)\n",
        "        self.num_gts = 0\n",
        "        self.precision_recall = dict()\n",
        "\n",
        "    def copy(self):\n",
        "        copy_instance = ModelEvaluator(self.tracker_id)\n",
        "        copy_instance.conf_scores = self.conf_scores.copy()\n",
        "        copy_instance.tp_indicator = self.tp_indicator.copy()\n",
        "        copy_instance.match_idx = self.match_idx.copy()\n",
        "        copy_instance.num_gts = self.num_gts\n",
        "        copy_instance.precision_recall = self.precision_recall.copy()\n",
        "\n",
        "        return copy_instance\n",
        "\n",
        "    def update_metrics(self, conf_scores, tp_indicator, match_idx, num_gt_batch):\n",
        "        conf_scores = conf_scores.detach().cpu().numpy() if isinstance(conf_scores, torch.Tensor) else conf_scores\n",
        "        tp_indicator = tp_indicator.detach().cpu().numpy() if isinstance(tp_indicator, torch.Tensor) else tp_indicator\n",
        "        match_idx = match_idx.detach().cpu().numpy() if isinstance(match_idx, torch.Tensor) else match_idx\n",
        "\n",
        "        assert isinstance(conf_scores, np.ndarray)\n",
        "        assert isinstance(tp_indicator, np.ndarray)\n",
        "        assert isinstance(match_idx, np.ndarray)\n",
        "        assert len(conf_scores) == len(tp_indicator)\n",
        "        assert len(match_idx) == len(tp_indicator)\n",
        "\n",
        "        if len(match_idx) == 0:\n",
        "            self.num_gts += num_gt_batch\n",
        "            return\n",
        "\n",
        "        if num_gt_batch == 0:\n",
        "            assert np.all(tp_indicator == False)\n",
        "        else:\n",
        "            assert np.max(match_idx) < num_gt_batch\n",
        "\n",
        "        self.conf_scores = np.concatenate([self.conf_scores, conf_scores])\n",
        "        self.tp_indicator = np.concatenate([self.tp_indicator, tp_indicator])\n",
        "        match_idx_new = match_idx + self.num_gts\n",
        "\n",
        "        if self.tracker_id == 0:\n",
        "            debug = True\n",
        "\n",
        "        self.match_idx = np.concatenate([self.match_idx, match_idx_new])\n",
        "        self.num_gts += num_gt_batch\n",
        "\n",
        "        for i in range(match_idx_new.shape[0]):\n",
        "            if tp_indicator[i] and (match_idx_new[i] >= self.num_gts):\n",
        "                raise RuntimeError(\"Check Failed\")\n",
        "\n",
        "    def compute(self):\n",
        "        precision_recall = {}\n",
        "        n_tp, n_tot_pred, num_gts = 0.0, 0.0, self.num_gts\n",
        "\n",
        "        sorted_idx = np.argsort(self.conf_scores)\n",
        "        sorted_scores, sorted_tp_indicator, sorted_match_idx = self.conf_scores[sorted_idx], self.tp_indicator[sorted_idx], self.match_idx[sorted_idx]\n",
        "\n",
        "        for score, is_tp, match_idx in zip(sorted_scores, sorted_tp_indicator, sorted_match_idx):\n",
        "            n_tot_pred += 1\n",
        "            if is_tp:\n",
        "                n_tp += 1\n",
        "            precision = n_tp / n_tot_pred\n",
        "            recall = np.sum(sorted_tp_indicator[:n_tot_pred]) / num_gts\n",
        "            precision_recall[recall] = precision\n",
        "\n",
        "        self.precision_recall = precision_recall\n",
        "\n",
        "        del sorted_scores, sorted_tp_indicator, sorted_match_idx\n",
        "        return precision_recall\n",
        "\n",
        "\n",
        "    def sort_curve(self):\n",
        "        assert len(self.precision_recall) != 0, \"[ERROR] Please compute precision and recall before calling sort_curve\"\n",
        "        recall_sorted = sorted(list(self.precision_recall.keys()))\n",
        "        precision_sorted = [self.precision_recall[k] for k in recall_sorted]\n",
        "        return recall_sorted, precision_sorted\n",
        "\n",
        "    def compute_average_precision(self):\n",
        "        assert len(self.precision_recall) != 0, \"[ERROR] Please compute precision and recall before calling compute_average_precision\"\n",
        "        recall_sorted = sorted(list(self.precision_recall.keys()))\n",
        "        precision_sorted = [self.precision_recall[k] for k in recall_sorted]\n",
        "        if len(recall_sorted) > 1:\n",
        "            return auc(recall_sorted, precision_sorted)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "os.makedirs(\"infer_result\", exist_ok=True)\n",
        "\n",
        "eval_epoch = 20\n",
        "VISUALIZATION = True\n",
        "batch_size = 2\n",
        "cate_thresh = 0.33\n",
        "\n",
        "resnet50_fpn = Resnet50Backbone()\n",
        "solo_head = SOLOHead(num_classes=4)\n",
        "\n",
        "solo_head.postprocess_cfg['cate_thresh'] = cate_thresh\n",
        "print(f\"[INFO] Category Threshold: {cate_thresh}\")\n",
        "\n",
        "print(f\"[INFO] eval epoch: {eval_epoch}\")\n",
        "checkpoint_file = f\"./train_checkpt/solo_epoch_{eval_epoch}\"\n",
        "checkpoint = torch.load(checkpoint_file)\n",
        "print(\"[INFO] Loaded checkpoint file successfully\")\n",
        "solo_head.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "solo_head = solo_head.to(device)\n",
        "solo_head.eval()\n",
        "\n",
        "color_list = [\"jet\", \"ocean\", \"Spectral\"]\n",
        "\n",
        "model_evals = []\n",
        "for i in range(solo_head.num_classes - 1):\n",
        "    model_evals.append(ModelEvaluator(i))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for iter, data in enumerate(tqdm(test_loader), 0):\n",
        "        img, label_list, mask_list, bbox_list = [\n",
        "            data[i] for i in range(len(data))]\n",
        "        img = img.to(device)\n",
        "        label_list = [x.to(device) for x in label_list]\n",
        "        mask_list = [x.to(device) for x in mask_list]\n",
        "        bbox_list = [x.to(device) for x in bbox_list]\n",
        "\n",
        "        backout = resnet50_fpn(img)\n",
        "        fpn_feat_list = list(backout.values())\n",
        "        cate_pred_list, ins_pred_list = solo_head.forward(\n",
        "            fpn_feat_list, eval=True)\n",
        "\n",
        "        NMS_sorted_scores_list, NMS_sorted_cate_label_list, NMS_sorted_ins_list = solo_head.PostProcess(\n",
        "            ins_pred_list, cate_pred_list, (img.shape[2], img.shape[3]))\n",
        "        del ins_pred_list\n",
        "        del cate_pred_list\n",
        "        if (VISUALIZATION):\n",
        "            solo_head.PlotInfer(NMS_sorted_scores_list, NMS_sorted_cate_label_list, NMS_sorted_ins_list,\n",
        "                                color_list, img, iter)\n",
        "\n",
        "        for sorted_scores, sorted_cate_label, sorted_ins, label_gt, mask_gt in zip(NMS_sorted_scores_list,\n",
        "                                                                                   NMS_sorted_cate_label_list,\n",
        "                                                                                   NMS_sorted_ins_list, label_list,\n",
        "                                                                                   mask_list):\n",
        "            assert torch.all(sorted_cate_label != 0)\n",
        "            for label_x in range(solo_head.num_classes):\n",
        "                if (label_x == 0):\n",
        "                    continue\n",
        "                idx_gt = (label_gt == label_x)\n",
        "                idx_pred = (sorted_cate_label == label_x)\n",
        "\n",
        "                N_gt = torch.sum(idx_gt).item()\n",
        "                N_pred = torch.sum(idx_pred).item()\n",
        "                if N_gt == 0:\n",
        "                    if N_pred == 0:\n",
        "                        continue\n",
        "                    conf_scores = sorted_scores[idx_pred]\n",
        "                    tp_indicator = np.zeros((N_pred, ), dtype=np.bool_)\n",
        "                    match_idx = np.zeros((N_pred, ), dtype=np.int32)\n",
        "                else:\n",
        "                    if N_pred != 0:\n",
        "                        conf_scores = sorted_scores[idx_pred]\n",
        "                        cate_label = sorted_cate_label[idx_pred]\n",
        "                        ins_pred = sorted_ins[idx_pred]\n",
        "                        assert len(ins_pred) == N_pred\n",
        "\n",
        "                        ins_gt = mask_gt[idx_gt]\n",
        "\n",
        "                        ious = solo_head.MatrixIOU(ins_pred > solo_head.postprocess_cfg['ins_thresh'],\n",
        "                                                   ins_gt > solo_head.postprocess_cfg['ins_thresh'])\n",
        "                        assert ious.shape == (N_pred, N_gt)\n",
        "                        max_ious, iou_max_idx = torch.max(ious, dim=1)\n",
        "                        assert max_ious.shape[0] == N_pred\n",
        "                        assert iou_max_idx.shape[0] == N_pred\n",
        "\n",
        "                        tp_indicator = (\n",
        "                            max_ious > solo_head.postprocess_cfg['IoU_thresh'])\n",
        "                        match_idx = iou_max_idx\n",
        "                    else:\n",
        "                        conf_scores = torch.zeros((0,))\n",
        "                        tp_indicator = torch.zeros((0,), dtype=torch.bool)\n",
        "                        match_idx = torch.zeros((0,), dtype=torch.int)\n",
        "\n",
        "                tmp = model_evals[label_x - 1]\n",
        "                if isinstance(tmp, ModelEvaluator):\n",
        "                    tmp.update_metrics(conf_scores, tp_indicator,\n",
        "                                  match_idx, N_gt)\n",
        "                    model_evals[label_x - 1] = tmp.copy()\n",
        "                else:\n",
        "                    print(\"tmp is not of type ModelEvaluator\")\n",
        "\n",
        "        output_filename = os.path.join(\"output\", f\"results_iter_{iter}.txt\")\n",
        "        with open(output_filename, 'w') as output_file:\n",
        "            output_file.write(f\"Results of iter {iter}\\n\")\n",
        "            for i in range(solo_head.num_classes - 1):\n",
        "                tmp = model_evals[i]\n",
        "                if isinstance(tmp, ModelEvaluator):\n",
        "                    tmp.compute()\n",
        "                    if not tmp.precision_recall:\n",
        "                        continue\n",
        "                    recall, precision = tmp.sort_curve()\n",
        "                    average_precision = tmp.compute_average_precision()\n",
        "                    output_file.write(f\"class_id: {i}. average_precision: {average_precision}\\n\")\n",
        "                    tmp.reset()\n",
        "                else:\n",
        "                    print(\"tmp is not of type ModelEvaluator\")\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    for i in range(solo_head.num_classes - 1):\n",
        "        tmp = model_evals[i]\n",
        "        if isinstance(tmp, ModelEvaluator):\n",
        "            tmp.compute()\n",
        "            recall, precision = tmp.sort_curve()\n",
        "\n",
        "            plt.plot(recall, precision)\n",
        "\n",
        "            average_precision = tmp.compute_average_precision()\n",
        "            print(f\"class_id: {i}. average_precision: {average_precision}\")\n",
        "            tmp.reset()\n",
        "\n",
        "        else:\n",
        "            print(\"tmp is not of type ModelEvaluator\")\n"
      ],
      "metadata": {
        "id": "tdmlfF2Nqr1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses[\"total\"], label=\"Train Total Loss\")\n",
        "plt.plot(train_losses[\"cate\"], label=\"Train Category Loss\")\n",
        "plt.plot(train_losses[\"mask\"], label=\"Train Mask Loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(val_losses[\"total\"], label=\"Validation Total Loss\")\n",
        "plt.plot(val_losses[\"cate\"], label=\"Validation Category Loss\")\n",
        "plt.plot(val_losses[\"mask\"], label=\"Validation Mask Loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FLt_wbUgcTYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}